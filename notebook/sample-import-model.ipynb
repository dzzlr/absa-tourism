{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5919c5f",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00b37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import Sastrawi\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d986b5e",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bf8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translate_text(text):\n",
    "    keyword = ['(Diterjemahkan oleh Google)', '(Asli)']\n",
    "    if keyword[0] in text and keyword[1] in text:\n",
    "        text = text[text.find(keyword[0]) + len(keyword[0]):text.rfind(keyword[1])]\n",
    "    elif keyword[0] in text and keyword[1] not in text:\n",
    "        text = text[text.find(keyword[0]) + len(keyword[0]):]\n",
    "    return text\n",
    "\n",
    "def reduction_spelling_error(text):\n",
    "    if re.search(r'([a-zA-Z])\\1{2,}', text): # Spell\n",
    "        text = re.sub(r'([a-zA-Z])\\1{2,}','\\\\1', text)\n",
    "    return text\n",
    "\n",
    "def ws_remove(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def pt_remove(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'[0-9]', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def stop(text):\n",
    "    stop_factory = StopWordRemoverFactory().get_stop_words() #load default stopword\n",
    "    more_stopword = ['daring', 'online','yg'] #menambahkan stopword\n",
    "\n",
    "    data = stop_factory + more_stopword #menggabungkan stopword\n",
    "\n",
    "    dictionary = ArrayDictionary(data)\n",
    "    str = StopWordRemover(dictionary)\n",
    "    return str.remove(text)\n",
    "\n",
    "def stemmer(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmerr = factory.create_stemmer()\n",
    "    return stemmerr.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32690d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoji\n",
    "# import emoji\n",
    "def convert_unicode(text):\n",
    "    text= repr(text.encode('ascii',errors='backslashreplace'))\n",
    "    text= text.replace(\"\\\\U000\",\" 0x\")\n",
    "    # text = re.sub('\\\\', '', text)\n",
    "    text=text[2:-1]\n",
    "    text_tokens = text.split(\" \")\n",
    "    new_text = \"\"\n",
    "    for i in text_tokens:\n",
    "        # print(i[:1])\n",
    "        if i[:1] == '\\\\':\n",
    "            continue\n",
    "        else:\n",
    "            new_text += \" \" + i\n",
    "    return new_text\n",
    "# df['text_unicode'] = df['text_ws_remove'].apply(convert_unicode)\n",
    "df_emoji= pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTDw-1PPBpNDwXch3yb3ApjszyJ1dX5iAOKQWnjDsdEqpzVAaiM9cOduOk63A8WGSTwz_HFZbI5ZUGR/pub?gid=1445261330&single=true&output=csv')# df_emoji.head()\n",
    "\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (df_emoji['Negative'] > df_emoji['Neutral']) & (df_emoji['Negative'] >= df_emoji['Positive']),\n",
    "    (df_emoji['Negative'] < df_emoji['Neutral']) & (df_emoji['Neutral'] >= df_emoji['Positive']),\n",
    "    (df_emoji['Neutral'] < df_emoji['Positive']) & (df_emoji['Negative'] <= df_emoji['Positive'])\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['negatif', 'netral', 'positif']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df_emoji['sentimen'] = np.select(conditions, values)\n",
    "\n",
    "# display updated DataFrame\n",
    "# df_emoji.head()\n",
    "#drop unused column\n",
    "df_emoji=df_emoji.drop(columns=['Emoji',\t'Occurrences',\t'Position'\t,'Negative',\t'Neutral',\t'Positive',\t'Unicode name',\t'Unicode block'])\n",
    "# df_emoji\n",
    "# df_emoji['Unicode codepoint'] = df_emoji['Unicode codepoint'].apply(clean_text)\n",
    "df_emoji=df_emoji.set_index(['Unicode codepoint'])\n",
    "# df_emoji.head()\n",
    "dict_emoji=df_emoji.to_dict()\n",
    "dict_emoji=dict_emoji['sentimen']\n",
    "# dict_emoji\n",
    "# text = 'Perjalanan yg bnr\" bikin Syahduu ,, dr Tempat parkir ke lokasi Curug ,, Lelahh pun terasa hilang ketika sampe lokasi ,, ðŸ¥°ðŸ¥° â€¦'\n",
    "def convert_emoji(text):\n",
    "    text_tokens = text.split(\" \")\n",
    "    new_text = \"\"\n",
    "    for i in text_tokens:\n",
    "        if i[:2] =='0x':\n",
    "            if i in dict_emoji:\n",
    "                new_text +=  \" \" +dict_emoji[i]\n",
    "            else:\n",
    "              i=''\n",
    "              new_text+=i\n",
    "        else:\n",
    "            new_text += \" \" + i\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a929b7",
   "metadata": {},
   "source": [
    "# New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5e480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teks='''Barusan dari situ dan pas ke toiletnya ampun kotor banget,dan bau pesing,tidak terawat padahal berbayar.\n",
    "Kalah sama toilet pom bensin yang gratis.\n",
    "Udah gitu pemandangannya biasa aja.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435e17b9-5b3b-4256-ac62-9770e7df532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teks='Barusan dari situ dan pas ke toiletnya ampun kotor banget,dan bau pesing,tidak terawat padahal berbayar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20830a67",
   "metadata": {},
   "source": [
    "# Pre-processing New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21327856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barusan situ pas toilet ampun kotor banget bau pesing awat bayar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teks=get_translate_text(teks)\n",
    "teks=reduction_spelling_error(teks)\n",
    "teks=convert_unicode(teks)\n",
    "teks=convert_emoji(teks)\n",
    "teks=pt_remove(teks)\n",
    "teks=teks.lower()\n",
    "teks=stop(teks)\n",
    "teks=stemmer(teks)\n",
    "teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "448349fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6fcf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barusan situ pas toilet ampun kotor banget bau pesing awat bayar']\n"
     ]
    }
   ],
   "source": [
    "print([teks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65b1cc",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "577c266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fakhri\\AppData\\Local\\Temp\\ipykernel_7332\\262096972.py:2: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  vectorizer = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../model/vectorizer.pkl\", 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffab523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teks=df_test.values.tolist()\n",
    "# teks=['kali','dua','seperti','biasa','sejuk','ramai,jalan,puncak,sedikit,kurang,bagus,tiket,weekend,40an,rb,1,orang,1,sepeda,motor,fasilitas,perlu,tingkat,untuk,beberapa,toilet,itu,udh,rapi,aman']\n",
    "teks=vectorizer.transform([teks])\n",
    "# teks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07bf7cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x390 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b9c6e",
   "metadata": {},
   "source": [
    "# Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095ca972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Columns: 390 entries, acara to yg\n",
      "dtypes: float64(390)\n",
      "memory usage: 3.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_teks = pd.DataFrame(teks.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df_teks.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b19b0",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eaca711",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model/model.pkl\", 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2d0980",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c610764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predict = clf.predict(df_teks)\n",
    "sample_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e624638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9971214 , 0.00124123, 0.00163737]], dtype=float32),\n",
       " array([[0.01576329, 0.01301333, 0.9712234 ]], dtype=float32),\n",
       " array([[0.5490253 , 0.09777901, 0.35319573]], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_proba = clf.predict_proba(df_teks)\n",
    "sample_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d075f2-f61b-43d8-bd04-e2754897b360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b549cc5d-4d95-4e91-abbe-9a9a8fb8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [\n",
    "    {\n",
    "        'label': 'aksesibilitas',\n",
    "        'score': {\n",
    "            'neutral': round(sample_proba[0][0][0], 3),\n",
    "            'positive': round(sample_proba[0][0][1], 3),\n",
    "            'negative': round(sample_proba[0][0][2], 3)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'label': 'fasilitas',\n",
    "        'score': {\n",
    "            'neutral': round(sample_proba[1][0][0], 3),\n",
    "            'positive': round(sample_proba[1][0][1], 3),\n",
    "            'negative': round(sample_proba[1][0][2], 3)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'label': 'aktivitas',\n",
    "        'score': {\n",
    "            'neutral': round(sample_proba[2][0][0], 3),\n",
    "            'positive': round(sample_proba[2][0][1], 3),\n",
    "            'negative': round(sample_proba[2][0][2], 3)\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e209f73-c920-43e2-b144-ee63a9315419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'aksesibilitas',\n",
       "  'score': {'neutral': 0.997, 'positive': 0.001, 'negative': 0.002}},\n",
       " {'label': 'fasilitas',\n",
       "  'score': {'neutral': 0.016, 'positive': 0.013, 'negative': 0.971}},\n",
       " {'label': 'aktivitas',\n",
       "  'score': {'neutral': 0.549, 'positive': 0.098, 'negative': 0.353}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b41ce",
   "metadata": {},
   "source": [
    "# Convert Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e33b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampel=sample_predict.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d4e9432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sampel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted=[]\n",
    "for i in sampel:\n",
    "    for j in i:\n",
    "        if j == 0:\n",
    "            converted.append('Netral')\n",
    "        elif j == 1: \n",
    "            converted.append('Positif')\n",
    "        else:\n",
    "            converted.append('Negatif')\n",
    "print(\"Aksesibilitas\\t\",converted[0])\n",
    "print(\"Fasilitas\\t\",converted[1])\n",
    "print(\"Aktivitas\\t\",converted[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
